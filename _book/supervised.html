<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Supervised Learning | R (BGU course)</title>
  <meta name="description" content="Class notes for the R course at the BGU MGMT faculty." />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Supervised Learning | R (BGU course)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Class notes for the R course at the BGU MGMT faculty." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Supervised Learning | R (BGU course)" />
  
  <meta name="twitter:description" content="Class notes for the R course at the BGU MGMT faculty." />
  

<meta name="author" content="Jonathan D. Rosenblatt" />


<meta name="date" content="2019-06-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multivariate.html">
<link rel="next" href="plotting.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<link href="libs/sequences-0.1/sequences.css" rel="stylesheet" />
<script src="libs/sunburst-binding-2.1.0/sunburst.js"></script>
<script src="libs/d3-5.7.0/d3.min.js"></script>
<script src="libs/d3-lasso-0.0.5/d3-lasso.min.js"></script>
<link href="libs/ggiraphjs-0.1.0/styles.css" rel="stylesheet" />
<script src="libs/ggiraphjs-0.1.0/ggiraphjs.min.js"></script>
<script src="libs/girafe-binding-0.6.0/girafe.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Course</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#notation-conventions"><i class="fa fa-check"></i><b>1.1</b> Notation Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#ecosystem"><i class="fa fa-check"></i><b>2.2</b> The R Ecosystem</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#bibliographic-notes"><i class="fa fa-check"></i><b>2.3</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>3</b> R Basics</a><ul>
<li class="chapter" data-level="3.0.1" data-path="basics.html"><a href="basics.html#other-ides"><i class="fa fa-check"></i><b>3.0.1</b> Other IDEs</a></li>
<li class="chapter" data-level="3.1" data-path="basics.html"><a href="basics.html#file-types"><i class="fa fa-check"></i><b>3.1</b> File types</a></li>
<li class="chapter" data-level="3.2" data-path="basics.html"><a href="basics.html#simple-calculator"><i class="fa fa-check"></i><b>3.2</b> Simple calculator</a></li>
<li class="chapter" data-level="3.3" data-path="basics.html"><a href="basics.html#probability-calculator"><i class="fa fa-check"></i><b>3.3</b> Probability calculator</a></li>
<li class="chapter" data-level="3.4" data-path="basics.html"><a href="basics.html#getting-help"><i class="fa fa-check"></i><b>3.4</b> Getting Help</a></li>
<li class="chapter" data-level="3.5" data-path="basics.html"><a href="basics.html#variable-asignment"><i class="fa fa-check"></i><b>3.5</b> Variable Asignment</a></li>
<li class="chapter" data-level="3.6" data-path="basics.html"><a href="basics.html#missing"><i class="fa fa-check"></i><b>3.6</b> Missing</a></li>
<li class="chapter" data-level="3.7" data-path="basics.html"><a href="basics.html#piping"><i class="fa fa-check"></i><b>3.7</b> Piping</a></li>
<li class="chapter" data-level="3.8" data-path="basics.html"><a href="basics.html#vector-creation-and-manipulation"><i class="fa fa-check"></i><b>3.8</b> Vector Creation and Manipulation</a></li>
<li class="chapter" data-level="3.9" data-path="basics.html"><a href="basics.html#search-paths-and-packages"><i class="fa fa-check"></i><b>3.9</b> Search Paths and Packages</a></li>
<li class="chapter" data-level="3.10" data-path="basics.html"><a href="basics.html#simple-plotting"><i class="fa fa-check"></i><b>3.10</b> Simple Plotting</a></li>
<li class="chapter" data-level="3.11" data-path="basics.html"><a href="basics.html#object-types"><i class="fa fa-check"></i><b>3.11</b> Object Types</a></li>
<li class="chapter" data-level="3.12" data-path="basics.html"><a href="basics.html#data-frames"><i class="fa fa-check"></i><b>3.12</b> Data Frames</a></li>
<li class="chapter" data-level="3.13" data-path="basics.html"><a href="basics.html#exctraction"><i class="fa fa-check"></i><b>3.13</b> Exctraction</a></li>
<li class="chapter" data-level="3.14" data-path="basics.html"><a href="basics.html#augmentations-of-the-data.frame-class"><i class="fa fa-check"></i><b>3.14</b> Augmentations of the data.frame class</a></li>
<li class="chapter" data-level="3.15" data-path="basics.html"><a href="basics.html#data-import-and-export"><i class="fa fa-check"></i><b>3.15</b> Data Import and Export</a><ul>
<li class="chapter" data-level="3.15.1" data-path="basics.html"><a href="basics.html#import-from-web"><i class="fa fa-check"></i><b>3.15.1</b> Import from WEB</a></li>
<li class="chapter" data-level="3.15.2" data-path="basics.html"><a href="basics.html#import-from-clipboard"><i class="fa fa-check"></i><b>3.15.2</b> Import From Clipboard</a></li>
<li class="chapter" data-level="3.15.3" data-path="basics.html"><a href="basics.html#export-as-csv"><i class="fa fa-check"></i><b>3.15.3</b> Export as CSV</a></li>
<li class="chapter" data-level="3.15.4" data-path="basics.html"><a href="basics.html#export-non-csv-files"><i class="fa fa-check"></i><b>3.15.4</b> Export non-CSV files</a></li>
<li class="chapter" data-level="3.15.5" data-path="basics.html"><a href="basics.html#reading-from-text-files"><i class="fa fa-check"></i><b>3.15.5</b> Reading From Text Files</a></li>
<li class="chapter" data-level="3.15.6" data-path="basics.html"><a href="basics.html#writing-data-to-text-files"><i class="fa fa-check"></i><b>3.15.6</b> Writing Data to Text Files</a></li>
<li class="chapter" data-level="3.15.7" data-path="basics.html"><a href="basics.html#xlsx-files"><i class="fa fa-check"></i><b>3.15.7</b> .XLS(X) files</a></li>
<li class="chapter" data-level="3.15.8" data-path="basics.html"><a href="basics.html#massive-files"><i class="fa fa-check"></i><b>3.15.8</b> Massive files</a></li>
<li class="chapter" data-level="3.15.9" data-path="basics.html"><a href="basics.html#databases"><i class="fa fa-check"></i><b>3.15.9</b> Databases</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="basics.html"><a href="basics.html#functions"><i class="fa fa-check"></i><b>3.16</b> Functions</a></li>
<li class="chapter" data-level="3.17" data-path="basics.html"><a href="basics.html#looping"><i class="fa fa-check"></i><b>3.17</b> Looping</a></li>
<li class="chapter" data-level="3.18" data-path="basics.html"><a href="basics.html#apply"><i class="fa fa-check"></i><b>3.18</b> Apply</a></li>
<li class="chapter" data-level="3.19" data-path="basics.html"><a href="basics.html#recursion"><i class="fa fa-check"></i><b>3.19</b> Recursion</a></li>
<li class="chapter" data-level="3.20" data-path="basics.html"><a href="basics.html#strings"><i class="fa fa-check"></i><b>3.20</b> Strings</a></li>
<li class="chapter" data-level="3.21" data-path="basics.html"><a href="basics.html#dates-and-times"><i class="fa fa-check"></i><b>3.21</b> Dates and Times</a><ul>
<li class="chapter" data-level="3.21.1" data-path="basics.html"><a href="basics.html#dates"><i class="fa fa-check"></i><b>3.21.1</b> Dates</a></li>
<li class="chapter" data-level="3.21.2" data-path="basics.html"><a href="basics.html#times"><i class="fa fa-check"></i><b>3.21.2</b> Times</a></li>
<li class="chapter" data-level="3.21.3" data-path="basics.html"><a href="basics.html#lubridate-package"><i class="fa fa-check"></i><b>3.21.3</b> lubridate Package</a></li>
</ul></li>
<li class="chapter" data-level="3.22" data-path="basics.html"><a href="basics.html#complex-objects"><i class="fa fa-check"></i><b>3.22</b> Complex Objects</a></li>
<li class="chapter" data-level="3.23" data-path="basics.html"><a href="basics.html#vectors-and-matrix-products"><i class="fa fa-check"></i><b>3.23</b> Vectors and Matrix Products</a></li>
<li class="chapter" data-level="3.24" data-path="basics.html"><a href="basics.html#bibliographic-notes-1"><i class="fa fa-check"></i><b>3.24</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="3.25" data-path="basics.html"><a href="basics.html#practice-yourself"><i class="fa fa-check"></i><b>3.25</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="datatable.html"><a href="datatable.html"><i class="fa fa-check"></i><b>4</b> data.table</a><ul>
<li class="chapter" data-level="4.1" data-path="datatable.html"><a href="datatable.html#make-your-own-variables"><i class="fa fa-check"></i><b>4.1</b> Make your own variables</a></li>
<li class="chapter" data-level="4.2" data-path="datatable.html"><a href="datatable.html#join"><i class="fa fa-check"></i><b>4.2</b> Join</a></li>
<li class="chapter" data-level="4.3" data-path="datatable.html"><a href="datatable.html#reshaping-data"><i class="fa fa-check"></i><b>4.3</b> Reshaping data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="datatable.html"><a href="datatable.html#wide-to-long"><i class="fa fa-check"></i><b>4.3.1</b> Wide to long</a></li>
<li class="chapter" data-level="4.3.2" data-path="datatable.html"><a href="datatable.html#long-to-wide"><i class="fa fa-check"></i><b>4.3.2</b> Long to wide</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="datatable.html"><a href="datatable.html#bibliographic-notes-2"><i class="fa fa-check"></i><b>4.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="4.5" data-path="datatable.html"><a href="datatable.html#practice-yourself-1"><i class="fa fa-check"></i><b>4.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="eda.html"><a href="eda.html#summary-statistics"><i class="fa fa-check"></i><b>5.1</b> Summary Statistics</a><ul>
<li class="chapter" data-level="5.1.1" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>5.1.1</b> Categorical Data</a></li>
<li class="chapter" data-level="5.1.2" data-path="eda.html"><a href="eda.html#continous-data"><i class="fa fa-check"></i><b>5.1.2</b> Continous Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="eda.html"><a href="eda.html#visualization"><i class="fa fa-check"></i><b>5.2</b> Visualization</a><ul>
<li class="chapter" data-level="5.2.1" data-path="eda.html"><a href="eda.html#categorical-data-1"><i class="fa fa-check"></i><b>5.2.1</b> Categorical Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="eda.html"><a href="eda.html#continuous-data"><i class="fa fa-check"></i><b>5.2.2</b> Continuous Data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="eda.html"><a href="eda.html#mixed-type-data"><i class="fa fa-check"></i><b>5.3</b> Mixed Type Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="eda.html"><a href="eda.html#alluvial"><i class="fa fa-check"></i><b>5.3.1</b> Alluvial Diagram</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="eda.html"><a href="eda.html#bibliographic-notes-3"><i class="fa fa-check"></i><b>5.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="5.5" data-path="eda.html"><a href="eda.html#practice-yourself-2"><i class="fa fa-check"></i><b>5.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>6</b> Linear Models</a><ul>
<li class="chapter" data-level="6.1" data-path="lm.html"><a href="lm.html#problem-setup"><i class="fa fa-check"></i><b>6.1</b> Problem Setup</a></li>
<li class="chapter" data-level="6.2" data-path="lm.html"><a href="lm.html#ols-estimation-in-r"><i class="fa fa-check"></i><b>6.2</b> OLS Estimation in R</a></li>
<li class="chapter" data-level="6.3" data-path="lm.html"><a href="lm.html#inference"><i class="fa fa-check"></i><b>6.3</b> Inference</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-coefficient"><i class="fa fa-check"></i><b>6.3.1</b> Testing a Hypothesis on a Single Coefficient</a></li>
<li class="chapter" data-level="6.3.2" data-path="lm.html"><a href="lm.html#constructing-a-confidence-interval-on-a-single-coefficient"><i class="fa fa-check"></i><b>6.3.2</b> Constructing a Confidence Interval on a Single Coefficient</a></li>
<li class="chapter" data-level="6.3.3" data-path="lm.html"><a href="lm.html#multiple-regression"><i class="fa fa-check"></i><b>6.3.3</b> Multiple Regression</a></li>
<li class="chapter" data-level="6.3.4" data-path="lm.html"><a href="lm.html#anova"><i class="fa fa-check"></i><b>6.3.4</b> ANOVA (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-contrast"><i class="fa fa-check"></i><b>6.3.5</b> Testing a Hypothesis on a Single Contrast (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lm.html"><a href="lm.html#bibliographic-notes-4"><i class="fa fa-check"></i><b>6.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="6.5" data-path="lm.html"><a href="lm.html#practice-yourself-3"><i class="fa fa-check"></i><b>6.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="glm.html"><a href="glm.html#problem-setup-1"><i class="fa fa-check"></i><b>7.1</b> Problem Setup</a></li>
<li class="chapter" data-level="7.2" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="glm.html"><a href="glm.html#logistic-regression-with-r"><i class="fa fa-check"></i><b>7.2.1</b> Logistic Regression with R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>7.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="7.4" data-path="glm.html"><a href="glm.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a></li>
<li class="chapter" data-level="7.5" data-path="glm.html"><a href="glm.html#bibliographic-notes-5"><i class="fa fa-check"></i><b>7.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="7.6" data-path="glm.html"><a href="glm.html#practice-glm"><i class="fa fa-check"></i><b>7.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lme.html"><a href="lme.html"><i class="fa fa-check"></i><b>8</b> Linear Mixed Models</a><ul>
<li class="chapter" data-level="8.1" data-path="lme.html"><a href="lme.html#problem-setup-2"><i class="fa fa-check"></i><b>8.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lme.html"><a href="lme.html#non-linear-mixed-models"><i class="fa fa-check"></i><b>8.1.1</b> Non-Linear Mixed Models</a></li>
<li class="chapter" data-level="8.1.2" data-path="lme.html"><a href="lme.html#generalized-linear-mixed-models-glmm"><i class="fa fa-check"></i><b>8.1.2</b> Generalized Linear Mixed Models (GLMM)</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lme.html"><a href="lme.html#mixed-models-with-r"><i class="fa fa-check"></i><b>8.2</b> Mixed Models with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lme.html"><a href="lme.html#a-single-random-effect"><i class="fa fa-check"></i><b>8.2.1</b> A Single Random Effect</a></li>
<li class="chapter" data-level="8.2.2" data-path="lme.html"><a href="lme.html#multiple-random-effects"><i class="fa fa-check"></i><b>8.2.2</b> Multiple Random Effects</a></li>
<li class="chapter" data-level="8.2.3" data-path="lme.html"><a href="lme.html#a-full-mixed-model"><i class="fa fa-check"></i><b>8.2.3</b> A Full Mixed-Model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lme.html"><a href="lme.html#serial"><i class="fa fa-check"></i><b>8.3</b> Serial Correlations</a></li>
<li class="chapter" data-level="8.4" data-path="lme.html"><a href="lme.html#extensions-1"><i class="fa fa-check"></i><b>8.4</b> Extensions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="lme.html"><a href="lme.html#cluster-robust-standard-errors"><i class="fa fa-check"></i><b>8.4.1</b> Cluster Robust Standard Errors</a></li>
<li class="chapter" data-level="8.4.2" data-path="lme.html"><a href="lme.html#linear-models-for-panel-data"><i class="fa fa-check"></i><b>8.4.2</b> Linear Models for Panel Data</a></li>
<li class="chapter" data-level="8.4.3" data-path="lme.html"><a href="lme.html#testing-hypotheses-on-correlations"><i class="fa fa-check"></i><b>8.4.3</b> Testing Hypotheses on Correlations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lme.html"><a href="lme.html#relation-to-other-estimators"><i class="fa fa-check"></i><b>8.5</b> Relation to Other Estimators</a><ul>
<li class="chapter" data-level="8.5.1" data-path="lme.html"><a href="lme.html#fixed-effects-in-the-econometric-literature"><i class="fa fa-check"></i><b>8.5.1</b> Fixed Effects in the Econometric Literature</a></li>
<li class="chapter" data-level="8.5.2" data-path="lme.html"><a href="lme.html#relation-to-generalized-least-squares-gls"><i class="fa fa-check"></i><b>8.5.2</b> Relation to Generalized Least Squares (GLS)</a></li>
<li class="chapter" data-level="8.5.3" data-path="lme.html"><a href="lme.html#relation-to-conditional-gaussian-fields"><i class="fa fa-check"></i><b>8.5.3</b> Relation to Conditional Gaussian Fields</a></li>
<li class="chapter" data-level="8.5.4" data-path="lme.html"><a href="lme.html#relation-to-empirical-risk-minimization-erm"><i class="fa fa-check"></i><b>8.5.4</b> Relation to Empirical Risk Minimization (ERM)</a></li>
<li class="chapter" data-level="8.5.5" data-path="lme.html"><a href="lme.html#relation-to-m-estimation"><i class="fa fa-check"></i><b>8.5.5</b> Relation to M-Estimation</a></li>
<li class="chapter" data-level="8.5.6" data-path="lme.html"><a href="lme.html#relation-to-generalize-estimating-equations-gee"><i class="fa fa-check"></i><b>8.5.6</b> Relation to Generalize Estimating Equations (GEE)</a></li>
<li class="chapter" data-level="8.5.7" data-path="lme.html"><a href="lme.html#manova"><i class="fa fa-check"></i><b>8.5.7</b> Relation to MANOVA</a></li>
<li class="chapter" data-level="8.5.8" data-path="lme.html"><a href="lme.html#relation-to-seemingly-unrelated-equations-sur"><i class="fa fa-check"></i><b>8.5.8</b> Relation to Seemingly Unrelated Equations (SUR)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lme.html"><a href="lme.html#bibliographic-notes-6"><i class="fa fa-check"></i><b>8.6</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="8.7" data-path="lme.html"><a href="lme.html#practice-yourself-4"><i class="fa fa-check"></i><b>8.7</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multivariate.html"><a href="multivariate.html"><i class="fa fa-check"></i><b>9</b> Multivariate Data Analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="multivariate.html"><a href="multivariate.html#signal-detection"><i class="fa fa-check"></i><b>9.1</b> Signal Detection</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multivariate.html"><a href="multivariate.html#hotellings-t2-test"><i class="fa fa-check"></i><b>9.1.1</b> Hotelling’s T2 Test</a></li>
<li class="chapter" data-level="9.1.2" data-path="multivariate.html"><a href="multivariate.html#various-types-of-signal-to-detect"><i class="fa fa-check"></i><b>9.1.2</b> Various Types of Signal to Detect</a></li>
<li class="chapter" data-level="9.1.3" data-path="multivariate.html"><a href="multivariate.html#simes-test"><i class="fa fa-check"></i><b>9.1.3</b> Simes’ Test</a></li>
<li class="chapter" data-level="9.1.4" data-path="multivariate.html"><a href="multivariate.html#signal-detection-with-r"><i class="fa fa-check"></i><b>9.1.4</b> Signal Detection with R</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multivariate.html"><a href="multivariate.html#signal-counting"><i class="fa fa-check"></i><b>9.2</b> Signal Counting</a></li>
<li class="chapter" data-level="9.3" data-path="multivariate.html"><a href="multivariate.html#identification"><i class="fa fa-check"></i><b>9.3</b> Signal Identification</a><ul>
<li class="chapter" data-level="9.3.1" data-path="multivariate.html"><a href="multivariate.html#signal-identification-in-r"><i class="fa fa-check"></i><b>9.3.1</b> Signal Identification in R</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="multivariate.html"><a href="multivariate.html#signal-estimation"><i class="fa fa-check"></i><b>9.4</b> Signal Estimation (*)</a></li>
<li class="chapter" data-level="9.5" data-path="multivariate.html"><a href="multivariate.html#bibliographic-notes-7"><i class="fa fa-check"></i><b>9.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="9.6" data-path="multivariate.html"><a href="multivariate.html#practice-yourself-5"><i class="fa fa-check"></i><b>9.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised.html"><a href="supervised.html"><i class="fa fa-check"></i><b>10</b> Supervised Learning</a><ul>
<li class="chapter" data-level="10.1" data-path="supervised.html"><a href="supervised.html#problem-setup-3"><i class="fa fa-check"></i><b>10.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="10.1.1" data-path="supervised.html"><a href="supervised.html#common-hypothesis-classes"><i class="fa fa-check"></i><b>10.1.1</b> Common Hypothesis Classes</a></li>
<li class="chapter" data-level="10.1.2" data-path="supervised.html"><a href="supervised.html#common-complexity-penalties"><i class="fa fa-check"></i><b>10.1.2</b> Common Complexity Penalties</a></li>
<li class="chapter" data-level="10.1.3" data-path="supervised.html"><a href="supervised.html#unbiased-risk-estimation"><i class="fa fa-check"></i><b>10.1.3</b> Unbiased Risk Estimation</a></li>
<li class="chapter" data-level="10.1.4" data-path="supervised.html"><a href="supervised.html#collecting-the-pieces"><i class="fa fa-check"></i><b>10.1.4</b> Collecting the Pieces</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="supervised.html"><a href="supervised.html#supervised-learning-in-r"><i class="fa fa-check"></i><b>10.2</b> Supervised Learning in R</a><ul>
<li class="chapter" data-level="10.2.1" data-path="supervised.html"><a href="supervised.html#least-squares"><i class="fa fa-check"></i><b>10.2.1</b> Linear Models with Least Squares Loss</a></li>
<li class="chapter" data-level="10.2.2" data-path="supervised.html"><a href="supervised.html#svm"><i class="fa fa-check"></i><b>10.2.2</b> SVM</a></li>
<li class="chapter" data-level="10.2.3" data-path="supervised.html"><a href="supervised.html#neural-nets"><i class="fa fa-check"></i><b>10.2.3</b> Neural Nets</a></li>
<li class="chapter" data-level="10.2.4" data-path="supervised.html"><a href="supervised.html#trees"><i class="fa fa-check"></i><b>10.2.4</b> Classification and Regression Trees (CART)</a></li>
<li class="chapter" data-level="10.2.5" data-path="supervised.html"><a href="supervised.html#k-nearest-neighbour-knn"><i class="fa fa-check"></i><b>10.2.5</b> K-nearest neighbour (KNN)</a></li>
<li class="chapter" data-level="10.2.6" data-path="supervised.html"><a href="supervised.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>10.2.6</b> Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="10.2.7" data-path="supervised.html"><a href="supervised.html#naive-bayes"><i class="fa fa-check"></i><b>10.2.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.2.8" data-path="supervised.html"><a href="supervised.html#random-forrest"><i class="fa fa-check"></i><b>10.2.8</b> Random Forrest</a></li>
<li class="chapter" data-level="10.2.9" data-path="supervised.html"><a href="supervised.html#boosting"><i class="fa fa-check"></i><b>10.2.9</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="supervised.html"><a href="supervised.html#bibliographic-notes-8"><i class="fa fa-check"></i><b>10.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="10.4" data-path="supervised.html"><a href="supervised.html#practice-yourself-6"><i class="fa fa-check"></i><b>10.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="plotting.html"><a href="plotting.html"><i class="fa fa-check"></i><b>11</b> Plotting</a><ul>
<li class="chapter" data-level="11.1" data-path="plotting.html"><a href="plotting.html#the-graphics-system"><i class="fa fa-check"></i><b>11.1</b> The graphics System</a><ul>
<li class="chapter" data-level="11.1.1" data-path="plotting.html"><a href="plotting.html#using-existing-plotting-functions"><i class="fa fa-check"></i><b>11.1.1</b> Using Existing Plotting Functions</a></li>
<li class="chapter" data-level="11.1.2" data-path="plotting.html"><a href="plotting.html#exporting-a-plot"><i class="fa fa-check"></i><b>11.1.2</b> Exporting a Plot</a></li>
<li class="chapter" data-level="11.1.3" data-path="plotting.html"><a href="plotting.html#fancy"><i class="fa fa-check"></i><b>11.1.3</b> Fancy graphics Examples</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="plotting.html"><a href="plotting.html#the-ggplot2-system"><i class="fa fa-check"></i><b>11.2</b> The ggplot2 System</a><ul>
<li class="chapter" data-level="11.2.1" data-path="plotting.html"><a href="plotting.html#extensions-of-the-ggplot2-system"><i class="fa fa-check"></i><b>11.2.1</b> Extensions of the ggplot2 System</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="plotting.html"><a href="plotting.html#interactive-graphics"><i class="fa fa-check"></i><b>11.3</b> Interactive Graphics</a><ul>
<li class="chapter" data-level="11.3.1" data-path="plotting.html"><a href="plotting.html#plotly"><i class="fa fa-check"></i><b>11.3.1</b> Plotly</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="plotting.html"><a href="plotting.html#other-r-interfaces-to-javascript-plotting"><i class="fa fa-check"></i><b>11.4</b> Other R Interfaces to JavaScript Plotting</a></li>
<li class="chapter" data-level="11.5" data-path="plotting.html"><a href="plotting.html#bibliographic-notes-9"><i class="fa fa-check"></i><b>11.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="11.6" data-path="plotting.html"><a href="plotting.html#practice-yourself-7"><i class="fa fa-check"></i><b>11.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>12</b> Reports</a><ul>
<li class="chapter" data-level="12.1" data-path="report.html"><a href="report.html#knitr"><i class="fa fa-check"></i><b>12.1</b> knitr</a><ul>
<li class="chapter" data-level="12.1.1" data-path="report.html"><a href="report.html#installation"><i class="fa fa-check"></i><b>12.1.1</b> Installation</a></li>
<li class="chapter" data-level="12.1.2" data-path="report.html"><a href="report.html#pandoc-markdown"><i class="fa fa-check"></i><b>12.1.2</b> Pandoc Markdown</a></li>
<li class="chapter" data-level="12.1.3" data-path="report.html"><a href="report.html#rmarkdown"><i class="fa fa-check"></i><b>12.1.3</b> Rmarkdown</a></li>
<li class="chapter" data-level="12.1.4" data-path="report.html"><a href="report.html#bibtex"><i class="fa fa-check"></i><b>12.1.4</b> BibTex</a></li>
<li class="chapter" data-level="12.1.5" data-path="report.html"><a href="report.html#compiling"><i class="fa fa-check"></i><b>12.1.5</b> Compiling</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="report.html"><a href="report.html#bookdown"><i class="fa fa-check"></i><b>12.2</b> bookdown</a></li>
<li class="chapter" data-level="12.3" data-path="report.html"><a href="report.html#shiny"><i class="fa fa-check"></i><b>12.3</b> Shiny</a><ul>
<li class="chapter" data-level="12.3.1" data-path="report.html"><a href="report.html#installation-1"><i class="fa fa-check"></i><b>12.3.1</b> Installation</a></li>
<li class="chapter" data-level="12.3.2" data-path="report.html"><a href="report.html#the-basics-of-shiny"><i class="fa fa-check"></i><b>12.3.2</b> The Basics of Shiny</a></li>
<li class="chapter" data-level="12.3.3" data-path="report.html"><a href="report.html#beyond-the-basics"><i class="fa fa-check"></i><b>12.3.3</b> Beyond the Basics</a></li>
<li class="chapter" data-level="12.3.4" data-path="report.html"><a href="report.html#shinydashboard"><i class="fa fa-check"></i><b>12.3.4</b> shinydashboard</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="report.html"><a href="report.html#flexdashboard"><i class="fa fa-check"></i><b>12.4</b> flexdashboard</a></li>
<li class="chapter" data-level="12.5" data-path="report.html"><a href="report.html#bibliographic-notes-10"><i class="fa fa-check"></i><b>12.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="12.6" data-path="report.html"><a href="report.html#practice-yourself-8"><i class="fa fa-check"></i><b>12.6</b> Practice Yourself</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R (BGU course)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Supervised Learning</h1>
<p>Machine learning is very similar to statistics, but it is certainly not the same. As the name suggests, in machine learning we want machines to learn. This means that we want to replace hard-coded expert algorithm, with data-driven self-learned algorithm.</p>
<p>There are many learning setups, that depend on what information is available to the machine. The most common setup, discussed in this chapter, is <em>supervised learning</em>. The name takes from the fact that by giving the machine data samples with known inputs (a.k.a. features) and desired outputs (a.k.a. labels), the human is effectively supervising the learning. If we think of the inputs as predictors, and outcomes as predicted, it is no wonder that supervised learning is very similar to statistical prediction. When asked “are these the same?” I like to give the example of internet fraud. If you take a sample of fraud “attacks”, a statistical formulation of the problem is highly unlikely. This is because fraud events are not randomly drawn from some distribution, but rather, arrive from an adversary learning the defenses and adapting to it. This instance of supervised learning is more similar to game theory than statistics.</p>
<p>Other types of machine learning problems include <span class="citation">(Sammut and Webb <a href="#ref-sammut2011encyclopedia">2011</a>)</span>:</p>
<ul>
<li><p><strong>Unsupervised Learning</strong>: Where we merely analyze the inputs/features, but no desirable outcome is available to the learning machine. See Chapter <a href="#unsupervised"><strong>??</strong></a>.</p></li>
<li><p><strong>Semi Supervised Learning</strong>: Where only part of the samples are labeled. A.k.a. <em>co-training</em>, <em>learning from labeled and unlabeled data</em>, <em>transductive learning</em>.</p></li>
<li><p><strong>Active Learning</strong>: Where the machine is allowed to query the user for labels. Very similar to <em>adaptive design of experiments</em>.</p></li>
<li><p><strong>Learning on a Budget</strong>: A version of active learning where querying for labels induces variable costs.</p></li>
<li><p><strong>Weak Learning</strong>: A version of supervised learning where the labels are given not by an expert, but rather by some heuristic rule. Example: mass-labeling cyber attacks by a rule based software, instead of a manual inspection.</p></li>
<li><p><strong>Reinforcement Learning</strong>:<br />
Similar to active learning, in that the machine may query for labels. Different from active learning, in that the machine does not receive labels, but <em>rewards</em>.</p></li>
<li><p><strong>Structure Learning</strong>: An instance of supervised learning where we predict objects with structure such as dependent vectors, graphs, images, tensors, etc.</p></li>
<li><p><strong>Online Learning</strong>: An instance of supervised learning, where we need to make predictions where data inputs as a stream.</p></li>
<li><p><strong>Transduction</strong>: An instance of supervised learning where we need to make predictions for a new set of predictors, but which are known at the time of learning. Can be thought of as semi-supervised <em>extrapolation</em>.</p></li>
<li><p><strong>Covariate shift</strong>: An instance of supervised learning where we need to make predictions for a set of predictors that ha a different distribution than the data generating source.</p></li>
<li><p><strong>Targeted Learning</strong>: A form of supervised learning, designed at causal inference for decision making.</p></li>
<li><p><strong>Co-training</strong>: An instance of supervised learning where we solve several problems, and exploit some assumed relation between the problems.</p></li>
<li><p><strong>Manifold learning</strong>: An instance of unsupervised learning, where the goal is to reduce the dimension of the data by embedding it into a lower dimensional manifold. A.k.a. <em>support estimation</em>.</p></li>
<li><p><strong>Similarity Learning</strong>: Where we try to learn how to measure similarity between objects (like faces, texts, images, etc.).</p></li>
<li><p><strong>Metric Learning</strong>: Like <em>similarity learning</em>, only that the similarity has to obey the definition of a <em>metric</em>.</p></li>
<li><p><strong>Learning to learn</strong>: Deals with the carriage of “experience” from one learning problem to another. A.k.a. <em>cummulative learning</em>, <em>knowledge transfer</em>, and <em>meta learning</em>.</p></li>
</ul>
<div id="problem-setup-3" class="section level2">
<h2><span class="header-section-number">10.1</span> Problem Setup</h2>
<p>We now present the <em>empirical risk minimization</em> (ERM) approach to supervised learning, a.k.a. <em>M-estimation</em> in the statistical literature.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> We do not discuss purely algorithmic approaches such as K-nearest neighbour and <em>kernel smoothing</em> due to space constraints. For a broader review of supervised learning, see the Bibliographic Notes.
</div>


<div class="example">
<span id="exm:rental-prices" class="example"><strong>Example 10.1  (Rental Prices)  </strong></span>Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation marks, number of recipients, etc.
</div>

<p>Given <span class="math inline">\(n\)</span> samples with inputs <span class="math inline">\(x\)</span> from some space <span class="math inline">\(\mathcal{X}\)</span> and desired outcome, <span class="math inline">\(y\)</span>, from some space <span class="math inline">\(\mathcal{Y}\)</span>. In our example, <span class="math inline">\(y\)</span> is the spam/no-spam label, and <span class="math inline">\(x\)</span> is a vector of the mail’s attributes. Samples, <span class="math inline">\((x,y)\)</span> have some distribution we denote <span class="math inline">\(P\)</span>. We want to learn a function that maps inputs to outputs, i.e., that classifies to spam given. This function is called a <em>hypothesis</em>, or <em>predictor</em>, denoted <span class="math inline">\(f\)</span>, that belongs to a hypothesis class <span class="math inline">\(\mathcal{F}\)</span> such that <span class="math inline">\(f:\mathcal{X} \to \mathcal{Y}\)</span>. We also choose some other function that fines us for erroneous prediction. This function is called the <em>loss</em>, and we denote it by <span class="math inline">\(l:\mathcal{Y}\times \mathcal{Y} \to \mathbb{R}^+\)</span>.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> The <em>hypothesis</em> in machine learning is only vaguely related the <em>hypothesis</em> in statistical testing, which is quite confusing.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> The <em>hypothesis</em> in machine learning is not a bona-fide <em>statistical model</em> since we don’t assume it is the data generating process, but rather some function which we choose for its good predictive performance.
</div>

<p>The fundamental task in supervised (statistical) learning is to recover a hypothesis that minimizes the average loss in the sample, and not in the population. This is know as the <em>risk minimization problem</em>.</p>

<div class="definition">
<span id="def:unnamed-chunk-236" class="definition"><strong>Definition 10.1  (Risk Function)  </strong></span>The <em>risk function</em>, a.k.a. <em>generalization error</em>, or <em>test error</em>, is the population average loss of a predictor <span class="math inline">\(f\)</span>:
<span class="math display">\[\begin{align}
  R(f):=\mathbb{E}_P[l(f(x),y)].
\end{align}\]</span>
</div>

The best predictor, is the risk minimizer:
<span class="math display" id="eq:risk">\[\begin{align}
  f^* := argmin_f \{R(f)\}.
  \tag{10.1}  
\end{align}\]</span>
<p>Another fundamental problem is that we do not know the distribution of all possible inputs and outputs, <span class="math inline">\(P\)</span>. We typically only have a sample of <span class="math inline">\((x_i,y_i), i=1,\dots,n\)</span>. We thus state the <em>empirical</em> counterpart of <a href="supervised.html#eq:risk">(10.1)</a>, which consists of minimizing the average loss. This is known as the <em>empirical risk miminization</em> problem (ERM).</p>

<div class="definition">
<span id="def:unnamed-chunk-237" class="definition"><strong>Definition 10.2  (Empirical Risk)  </strong></span>The <em>empirical risk function</em>, a.k.a. <em>in-sample error</em>, or <em>train error</em>, is the sample average loss of a predictor <span class="math inline">\(f\)</span>:
<span class="math display">\[\begin{align}
  R_n(f):= 1/n \sum_i l(f(x_i),y_i).
\end{align}\]</span>
</div>

A good candidate proxy for <span class="math inline">\(f^*\)</span> is its empirical counterpart, <span class="math inline">\(\hat f\)</span>, known as the <em>empirical risk minimizer</em>:
<span class="math display" id="eq:erm">\[\begin{align}
  \hat f := argmin_f \{ R_n(f) \}.
  \tag{10.2}  
\end{align}\]</span>
<p>To make things more explicit:</p>
<ul>
<li><span class="math inline">\(f\)</span> may be a linear function of the attributes, so that it may be indexed simply with its coefficient vector <span class="math inline">\(\beta\)</span>.</li>
<li><span class="math inline">\(l\)</span> may be a squared error loss: <span class="math inline">\(l(f(x),y):=(f(x)-y)^2\)</span>.</li>
</ul>
Under these conditions, the best predictor <span class="math inline">\(f^* \in \mathcal{F}\)</span> from problem <a href="supervised.html#eq:risk">(10.1)</a> is to
<span class="math display">\[\begin{align}
  f^* := argmin_\beta \{ \mathbb{E}_{P(x,y)}[(x&#39;\beta-y)^2] \}.
\end{align}\]</span>
When using a linear hypothesis with squared loss, we see that the empirical risk minimization problem collapses to an ordinary least-squares problem:
<span class="math display">\[\begin{align}
  \hat f := argmin_\beta \{1/n \sum_i (x_i&#39;\beta - y_i)^2 \}.
\end{align}\]</span>
<p>When data samples are assumingly independent, then maximum likelihood estimation is also an instance of ERM, when using the (negative) log likelihood as the loss function.</p>
<p>If we don’t assume any structure on the hypothesis, <span class="math inline">\(f\)</span>, then <span class="math inline">\(\hat f\)</span> from <a href="supervised.html#eq:erm">(10.2)</a> will interpolate the data, and <span class="math inline">\(\hat f\)</span> will be a very bad predictor. We say, it will <em>overfit</em> the observed data, and will have bad performance on new data.</p>
<p>We have several ways to avoid overfitting:</p>
<ol style="list-style-type: decimal">
<li>Restrict the hypothesis class <span class="math inline">\(\mathcal{F}\)</span> (such as linear functions).</li>
<li>Penalize for the complexity of <span class="math inline">\(f\)</span>. The penalty denoted by <span class="math inline">\(\Vert f \Vert\)</span>.</li>
<li>Unbiased risk estimation: <span class="math inline">\(R_n(f)\)</span> is not an unbiased estimator of <span class="math inline">\(R(f)\)</span>. Why? Think of estimating the mean with the sample minimum… Because <span class="math inline">\(R_n(f)\)</span> is downward biased, we may add some correction term, or compute <span class="math inline">\(R_n(f)\)</span> on different data than the one used to recover <span class="math inline">\(\hat f\)</span>.</li>
</ol>
<p>Almost all ERM algorithms consist of some combination of all the three methods above.</p>
<div id="common-hypothesis-classes" class="section level3">
<h3><span class="header-section-number">10.1.1</span> Common Hypothesis Classes</h3>
<p>Some common hypothesis classes, <span class="math inline">\(\mathcal{F}\)</span>, with restricted complexity, are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Linear hypotheses</strong>: such as linear models, GLMs, and (linear) support vector machines (SVM).</p></li>
<li><p><strong>Neural networks</strong>: a.k.a. <em>feed-forward</em> neural nets, <em>artificial</em> neural nets, and the celebrated class of <em>deep</em> neural nets.</p></li>
<li><strong>Tree</strong>: a.k.a. <em>decision rules</em>, is a class of hypotheses which can be stated as “if-then” rules.</li>
<li><p><strong>Reproducing Kernel Hilbert Space</strong>: a.k.a. RKHS, is a subset of “the space of all functions<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a>” that is both large enough to capture very complicated relations, but small enough so that it is less prone to overfitting, and also surprisingly simple to compute with.</p></li>
</ol>
</div>
<div id="common-complexity-penalties" class="section level3">
<h3><span class="header-section-number">10.1.2</span> Common Complexity Penalties</h3>
<p>The most common complexity penalty applies to classes that have a finite dimensional parametric representation, such as the class of linear predictors, parametrized via its coefficients <span class="math inline">\(\beta\)</span>. In such classes we may penalize for the norm of the parameters. Common penalties include:</p>
<ol style="list-style-type: decimal">
<li><strong>Ridge penalty</strong>: penalizing the <span class="math inline">\(l_2\)</span> norm of the parameter. I.e. <span class="math inline">\(\Vert f \Vert=\Vert \beta \Vert_2^2=\sum_j \beta_j^2\)</span>.</li>
<li><strong>LASSO penalty</strong>: penalizing the <span class="math inline">\(l_1\)</span> norm of the parameter. I.e., <span class="math inline">\(\Vert f \Vert=\Vert \beta \Vert_1=\sum_j |\beta_j|\)</span></li>
<li><strong>Elastic net</strong>: a combination of the lasso and ridge penalty. I.e. ,<span class="math inline">\(\Vert f \Vert= \alpha \Vert \beta \Vert_2^2 + (1-\alpha) \Vert \beta \Vert_1\)</span>.</li>
<li><strong>Function Norms</strong>: If the hypothesis class <span class="math inline">\(\mathcal{F}\)</span> does not admit a finite dimensional representation, the penalty is no longer a function of the parameters of the function. We may, however, penalize not the parametric representation of the function, but rather the function itself <span class="math inline">\(\Vert f \Vert=\sqrt{\int f(t)^2 dt}\)</span>.</li>
</ol>
</div>
<div id="unbiased-risk-estimation" class="section level3">
<h3><span class="header-section-number">10.1.3</span> Unbiased Risk Estimation</h3>
<p>The fundamental problem of overfitting, is that the empirical risk, <span class="math inline">\(R_n(\hat f)\)</span>, is downward biased to the population risk, <span class="math inline">\(R(\hat f)\)</span>. We can remove this bias in two ways: (a) purely algorithmic <em>resampling</em> approaches, and (b) theory driven estimators.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Train-Validate-Test</strong>: The simplest form of algorithmic validation is to split the data. A <em>train</em> set to train/estimate/learn <span class="math inline">\(\hat f\)</span>. A <em>validation</em> set to compute the out-of-sample expected loss, <span class="math inline">\(R(\hat f)\)</span>, and pick the best performing predictor. A <em>test</em> sample to compute the out-of-sample performance of the selected hypothesis. This is a very simple approach, but it is very “data inefficient”, thus motivating the next method.</p></li>
<li><p><strong>V-Fold Cross Validation</strong>: By far the most popular algorithmic unbiased risk estimator; in <em>V-fold CV</em> we “fold” the data into <span class="math inline">\(V\)</span> non-overlapping sets. For each of the <span class="math inline">\(V\)</span> sets, we learn <span class="math inline">\(\hat f\)</span> with the non-selected fold, and assess <span class="math inline">\(R(\hat f)\)</span>) on the selected fold. We then aggregate results over the <span class="math inline">\(V\)</span> folds, typically by averaging.</p></li>
<li><p><strong>AIC</strong>: Akaike’s information criterion (AIC) is a theory driven correction of the empirical risk, so that it is unbiased to the true risk. It is appropriate when using the likelihood loss.</p></li>
<li><p><strong>Cp</strong>: Mallow’s Cp is an instance of AIC for likelihood loss under normal noise.</p></li>
</ol>
<p>Other theory driven unbiased risk estimators include the <em>Bayesian Information Criterion</em> (BIC, aka SBC, aka SBIC), the <em>Minimum Description Length</em> (MDL), <em>Vapnic’s Structural Risk Minimization</em> (SRM), the <em>Deviance Information Criterion</em> (DIC), and the <em>Hannan-Quinn Information Criterion</em> (HQC).</p>
<p>Other resampling based unbiased risk estimators include resampling <strong>without replacement</strong> algorithms like <em>delete-d cross validation</em> with its many variations, and <strong>resampling with replacement</strong>, like the <em>bootstrap</em>, with its many variations.</p>
</div>
<div id="collecting-the-pieces" class="section level3">
<h3><span class="header-section-number">10.1.4</span> Collecting the Pieces</h3>
An ERM problem with regularization will look like
<span class="math display" id="eq:erm-regularized">\[\begin{align}
  \hat f := argmin_{f \in \mathcal{F}} \{ R_n(f)  + \lambda \Vert f \Vert \}.
  \tag{10.3}  
\end{align}\]</span>
<p>Collecting ideas from the above sections, a typical supervised learning pipeline will include: choosing the hypothesis class, choosing the penalty function and level, unbiased risk estimator. We emphasize that choosing the penalty function, <span class="math inline">\(\Vert f \Vert\)</span> is not enough, and we need to choose how “hard” to apply it. This if known as the <em>regularization level</em>, denoted by <span class="math inline">\(\lambda\)</span> in Eq.<a href="supervised.html#eq:erm-regularized">(10.3)</a>.</p>
<p>Examples of such combos include:</p>
<ol style="list-style-type: decimal">
<li>Linear regression, no penalty, train-validate test.</li>
<li>Linear regression, no penalty, AIC.</li>
<li>Linear regression, <span class="math inline">\(l_2\)</span> penalty, V-fold CV. This combo is typically known as <em>ridge regression</em>.</li>
<li>Linear regression, <span class="math inline">\(l_1\)</span> penalty, V-fold CV. This combo is typically known as <em>LASSO regression</em>.</li>
<li>Linear regression, <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span> penalty, V-fold CV. This combo is typically known as <em>elastic net regression</em>.</li>
<li>Logistic regression, <span class="math inline">\(l_2\)</span> penalty, V-fold CV.</li>
<li>SVM classification, <span class="math inline">\(l_2\)</span> penalty, V-fold CV.</li>
<li>Deep network, no penalty, V-fold CV.</li>
<li>Unrestricted, <span class="math inline">\(\Vert \partial^2 f \Vert_2\)</span>, V-fold CV. This combo is typically known as a <em>smoothing spline</em>.</li>
</ol>
<p>For fans of statistical hypothesis testing we will also emphasize: Testing and prediction are related, but are not the same:</p>
<ul>
<li>In the current chapter, we do not claim our models, <span class="math inline">\(f\)</span>, are generative. I.e., we do not claim that there is some causal relation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. We only claim that <span class="math inline">\(x\)</span> predicts <span class="math inline">\(y\)</span>.</li>
<li>It is possible that we will want to ignore a significant predictor, and add a non-significant one <span class="citation">(Foster and Stine <a href="#ref-foster2004variable">2004</a>)</span>.</li>
<li>Some authors will use hypothesis testing as an initial screening for candidate predictors. This is a useful heuristic, but that is all it is– a heuristic. It may also fail miserably if predictors are linearly dependent (a.k.a. multicollinear).</li>
</ul>
</div>
</div>
<div id="supervised-learning-in-r" class="section level2">
<h2><span class="header-section-number">10.2</span> Supervised Learning in R</h2>
<p>At this point, we have a rich enough language to do supervised learning with R.</p>
<p>In these examples, I will use two data sets from the <strong>ElemStatLearn</strong> package, that accompanies the seminal book by <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span>. I use the <code>spam</code> data for categorical predictions, and <code>prostate</code> for continuous predictions. In <code>spam</code> we will try to decide if a mail is spam or not. In <code>prostate</code> we will try to predict the size of a cancerous tumor. You can now call <code>?prostate</code> and <code>?spam</code> to learn more about these data sets.</p>
<p>Some boring pre-processing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Preparing prostate data</span>
<span class="kw">data</span>(<span class="st">&quot;prostate&quot;</span>, <span class="dt">package =</span> <span class="st">&#39;ElemStatLearn&#39;</span>)
prostate &lt;-<span class="st"> </span>data.table<span class="op">::</span><span class="kw">data.table</span>(prostate)
prostate.train &lt;-<span class="st"> </span>prostate[train<span class="op">==</span><span class="ot">TRUE</span>, <span class="op">-</span><span class="st">&quot;train&quot;</span>]
prostate.test &lt;-<span class="st"> </span>prostate[train<span class="op">!=</span><span class="ot">TRUE</span>, <span class="op">-</span><span class="st">&quot;train&quot;</span>]
y.train &lt;-<span class="st"> </span>prostate.train<span class="op">$</span>lcavol
X.train &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(prostate.train[, <span class="op">-</span><span class="st">&#39;lcavol&#39;</span>] )
y.test &lt;-<span class="st"> </span>prostate.test<span class="op">$</span>lcavol 
X.test &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(prostate.test[, <span class="op">-</span><span class="st">&#39;lcavol&#39;</span>] )

<span class="co"># Preparing spam data:</span>
<span class="kw">data</span>(<span class="st">&quot;spam&quot;</span>, <span class="dt">package =</span> <span class="st">&#39;ElemStatLearn&#39;</span>)
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(spam)
train.prop &lt;-<span class="st"> </span><span class="fl">0.66</span>
train.ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="ot">TRUE</span>,<span class="ot">FALSE</span>), 
                    <span class="dt">size =</span> n, 
                    <span class="dt">prob =</span> <span class="kw">c</span>(train.prop,<span class="dv">1</span><span class="op">-</span>train.prop), 
                    <span class="dt">replace=</span><span class="ot">TRUE</span>)
spam.train &lt;-<span class="st"> </span>spam[train.ind,]
spam.test &lt;-<span class="st"> </span>spam[<span class="op">!</span>train.ind,]

y.train.spam &lt;-<span class="st"> </span>spam.train<span class="op">$</span>spam
X.train.spam &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(spam.train[,<span class="kw">names</span>(spam.train)<span class="op">!=</span><span class="st">&#39;spam&#39;</span>] ) 
y.test.spam &lt;-<span class="st"> </span>spam.test<span class="op">$</span>spam
X.test.spam &lt;-<span class="st">  </span><span class="kw">as.matrix</span>(spam.test[,<span class="kw">names</span>(spam.test)<span class="op">!=</span><span class="st">&#39;spam&#39;</span>]) 

spam.dummy &lt;-<span class="st"> </span>spam
spam.dummy<span class="op">$</span>spam &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(spam<span class="op">$</span>spam<span class="op">==</span><span class="st">&#39;spam&#39;</span>) 
spam.train.dummy &lt;-<span class="st"> </span>spam.dummy[train.ind,]
spam.test.dummy &lt;-<span class="st"> </span>spam.dummy[<span class="op">!</span>train.ind,]</code></pre></div>
<p>We also define some utility functions that we will require down the road.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">l2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span>sum <span class="op">%&gt;%</span><span class="st"> </span>sqrt 
l1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">abs</span>(x) <span class="op">%&gt;%</span><span class="st"> </span>sum  
MSE &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span>mean 
missclassification &lt;-<span class="st"> </span><span class="cf">function</span>(tab) <span class="kw">sum</span>(tab[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)])<span class="op">/</span><span class="kw">sum</span>(tab)</code></pre></div>
<div id="least-squares" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Linear Models with Least Squares Loss</h3>
<p>The simplest approach to supervised learning, is simply with OLS: a linear predictor, squared error loss, and train-test risk estimator. Notice the better in-sample MSE than the out-of-sample. That is overfitting in action.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol<span class="op">~</span>. ,<span class="dt">data =</span> prostate.train)
<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ols.<span class="dv">1</span>)<span class="op">-</span>prostate.train<span class="op">$</span>lcavol) </code></pre></div>
<pre><code>## [1] 0.4383709</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ols.<span class="dv">1</span>, <span class="dt">newdata=</span>prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.5084068</code></pre>
<p>Things to note:</p>
<ul>
<li>I use the <code>newdata</code> argument of the <code>predict</code> function to make the out-of-sample predictions required to compute the test-error.</li>
<li>The test error is larger than the train error. That is overfitting in action.</li>
</ul>
<p>We now implement a V-fold CV, instead of our train-test approach. The assignment of each observation to each fold is encoded in <code>fold.assignment</code>. The following code is extremely inefficient, but easy to read.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">folds &lt;-<span class="st"> </span><span class="dv">10</span>
fold.assignment &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>folds, <span class="kw">nrow</span>(prostate), <span class="dt">replace =</span> <span class="ot">TRUE</span>)
errors &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>folds){
  prostate.cross.train &lt;-<span class="st"> </span>prostate[fold.assignment<span class="op">!=</span>k,] <span class="co"># train subset</span>
  prostate.cross.test &lt;-<span class="st">  </span>prostate[fold.assignment<span class="op">==</span>k,] <span class="co"># test subset</span>
  .ols &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol<span class="op">~</span>. ,<span class="dt">data =</span> prostate.cross.train) <span class="co"># train</span>
  .predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(.ols, <span class="dt">newdata=</span>prostate.cross.test)
  .errors &lt;-<span class="st">  </span>.predictions<span class="op">-</span>prostate.cross.test<span class="op">$</span>lcavol <span class="co"># save prediction errors in the fold</span>
  errors &lt;-<span class="st"> </span><span class="kw">c</span>(errors, .errors) <span class="co"># aggregate error over folds.</span>
}

<span class="co"># Cross validated prediction error:</span>
<span class="kw">MSE</span>(errors)</code></pre></div>
<pre><code>## [1] 0.5742128</code></pre>
<p>Let’s try all possible variable subsets, and choose the best performer with respect to the Cp criterion, which is an unbiased risk estimator. This is done with <code>leaps::regsubsets</code>. We see that the best performer has 3 predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">regfit.full &lt;-<span class="st"> </span>prostate.train <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>leaps<span class="op">::</span><span class="kw">regsubsets</span>(lcavol<span class="op">~</span>.,<span class="dt">data =</span> ., <span class="dt">method =</span> <span class="st">&#39;exhaustive&#39;</span>) <span class="co"># best subset selection</span>
<span class="kw">plot</span>(regfit.full, <span class="dt">scale =</span> <span class="st">&quot;Cp&quot;</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/all%20subset-1.png" width="50%" /></p>
<p>Things to note:</p>
<ul>
<li>The plot shows us which is the variable combination which is the best, i.e., has the smallest Cp.</li>
<li>Scanning over all variable subsets is impossible when the number of variables is large.</li>
</ul>
<p>Instead of the Cp criterion, we now compute the train and test errors for all the possible predictor subsets<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>. In the resulting plot we can see overfitting in action.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model.n &lt;-<span class="st"> </span>regfit.full <span class="op">%&gt;%</span><span class="st"> </span>summary <span class="op">%&gt;%</span><span class="st"> </span>length
X.train.named &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lcavol <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> prostate.train ) 
X.test.named &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lcavol <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> prostate.test ) 

val.errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, model.n)
train.errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, model.n)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>model.n) {
    coefi &lt;-<span class="st"> </span><span class="kw">coef</span>(regfit.full, <span class="dt">id =</span> i) <span class="co"># exctract coefficients of i&#39;th model</span>
    
    pred &lt;-<span class="st">  </span>X.train.named[, <span class="kw">names</span>(coefi)] <span class="op">%*%</span><span class="st"> </span>coefi <span class="co"># make in-sample predictions</span>
    train.errors[i] &lt;-<span class="st"> </span><span class="kw">MSE</span>(y.train <span class="op">-</span><span class="st"> </span>pred) <span class="co"># train errors</span>

    pred &lt;-<span class="st">  </span>X.test.named[, <span class="kw">names</span>(coefi)] <span class="op">%*%</span><span class="st"> </span>coefi <span class="co"># make out-of-sample predictions</span>
    val.errors[i] &lt;-<span class="st"> </span><span class="kw">MSE</span>(y.test <span class="op">-</span><span class="st"> </span>pred) <span class="co"># test errors</span>
}</code></pre></div>
<p>Plotting results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(train.errors, <span class="dt">ylab =</span> <span class="st">&quot;MSE&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&quot;o&quot;</span>)
<span class="kw">points</span>(val.errors, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Training&quot;</span>, <span class="st">&quot;Validation&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>), 
       <span class="dt">pch =</span> <span class="dv">19</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-239-1.png" width="50%" /></p>
<p>Checking all possible models is computationally very hard. <em>Forward selection</em> is a greedy approach that adds one variable at a time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols.<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol<span class="op">~</span><span class="dv">1</span> ,<span class="dt">data =</span> prostate.train)
model.scope &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">upper=</span>ols.<span class="dv">1</span>, <span class="dt">lower=</span>ols.<span class="dv">0</span>)
<span class="kw">step</span>(ols.<span class="dv">0</span>, <span class="dt">scope=</span>model.scope, <span class="dt">direction=</span><span class="st">&#39;forward&#39;</span>, <span class="dt">trace =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## Start:  AIC=30.1
## lcavol ~ 1
## 
##           Df Sum of Sq     RSS     AIC
## + lpsa     1    54.776  47.130 -19.570
## + lcp      1    48.805  53.101 -11.578
## + svi      1    35.829  66.077   3.071
## + pgg45    1    23.789  78.117  14.285
## + gleason  1    18.529  83.377  18.651
## + lweight  1     9.186  92.720  25.768
## + age      1     8.354  93.552  26.366
## &lt;none&gt;                 101.906  30.097
## + lbph     1     0.407 101.499  31.829
## 
## Step:  AIC=-19.57
## lcavol ~ lpsa
## 
##           Df Sum of Sq    RSS     AIC
## + lcp      1   14.8895 32.240 -43.009
## + svi      1    5.0373 42.093 -25.143
## + gleason  1    3.5500 43.580 -22.817
## + pgg45    1    3.0503 44.080 -22.053
## + lbph     1    1.8389 45.291 -20.236
## + age      1    1.5329 45.597 -19.785
## &lt;none&gt;                 47.130 -19.570
## + lweight  1    0.4106 46.719 -18.156
## 
## Step:  AIC=-43.01
## lcavol ~ lpsa + lcp
## 
##           Df Sum of Sq    RSS     AIC
## &lt;none&gt;                 32.240 -43.009
## + age      1   0.92315 31.317 -42.955
## + pgg45    1   0.29594 31.944 -41.627
## + gleason  1   0.21500 32.025 -41.457
## + lbph     1   0.13904 32.101 -41.298
## + lweight  1   0.05504 32.185 -41.123
## + svi      1   0.02069 32.220 -41.052</code></pre>
<pre><code>## 
## Call:
## lm(formula = lcavol ~ lpsa + lcp, data = prostate.train)
## 
## Coefficients:
## (Intercept)         lpsa          lcp  
##     0.08798      0.53369      0.38879</code></pre>
<p>Things to note:</p>
<ul>
<li>By default <code>step</code> add variables according to the <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a> criterion, which is a theory-driven unbiased risk estimator.</li>
<li>We need to tell <code>step</code> which is the smallest and largest models to consider using the <code>scope</code> argument.</li>
<li><code>direction='forward'</code> is used to “grow” from a small model. For “shrinking” a large model, use <code>direction='backward'</code>, or the default <code>direction='stepwise'</code>.</li>
</ul>
<p>We now learn a linear predictor on the <code>spam</code> data using, a least squares loss, and train-test risk estimator.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train the predictor</span>
ols.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(spam<span class="op">~</span>., <span class="dt">data =</span> spam.train.dummy) 

<span class="co"># make in-sample predictions</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(ols.<span class="dv">2</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> 
<span class="co"># inspect the confusion matrix</span>
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train.dummy<span class="op">$</span>spam)) </code></pre></div>
<pre><code>##           truth
## prediction    0    1
##      FALSE 1778  227
##      TRUE    66  980</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the train (in sample) misclassification</span>
<span class="kw">missclassification</span>(confusion.train) </code></pre></div>
<pre><code>## [1] 0.09603409</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># make out-of-sample prediction</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(ols.<span class="dv">2</span>, <span class="dt">newdata =</span> spam.test.dummy) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> 
<span class="co"># inspect the confusion matrix</span>
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test.dummy<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction   0   1
##      FALSE 884 139
##      TRUE   60 467</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the train (in sample) misclassification</span>
<span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1283871</code></pre>
<p>Things to note:</p>
<ul>
<li>I can use <code>lm</code> for categorical outcomes. <code>lm</code> will simply dummy-code the outcome.</li>
<li>A linear predictor trained on 0’s and 1’s will predict numbers. Think of these numbers as the probability of 1, and my prediction is the most probable class: <code>predicts()&gt;0.5</code>.</li>
<li>The train error is smaller than the test error. This is overfitting in action.</li>
</ul>
<p>The <code>glmnet</code> package is an excellent package that provides ridge, LASSO, and elastic net regularization, for all GLMs, so for linear models in particular.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">suppressMessages</span>(<span class="kw">library</span>(glmnet))

means &lt;-<span class="st"> </span><span class="kw">apply</span>(X.train, <span class="dv">2</span>, mean)
sds &lt;-<span class="st"> </span><span class="kw">apply</span>(X.train, <span class="dv">2</span>, sd)
X.train.scaled &lt;-<span class="st"> </span>X.train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sweep</span>(<span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">STATS =</span> means, <span class="dt">FUN =</span> <span class="st">`</span><span class="dt">-</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sweep</span>(<span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">STATS =</span> sds, <span class="dt">FUN =</span> <span class="st">`</span><span class="dt">/</span><span class="st">`</span>)

ridge.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x=</span>X.train.scaled, <span class="dt">y=</span>y.train, <span class="dt">family =</span> <span class="st">&#39;gaussian&#39;</span>, <span class="dt">alpha =</span> <span class="dv">0</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ridge.<span class="dv">2</span>, <span class="dt">newx =</span>X.train.scaled)<span class="op">-</span><span class="st"> </span>y.train)</code></pre></div>
<pre><code>## [1] 1.006028</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
X.test.scaled &lt;-<span class="st"> </span>X.test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sweep</span>(<span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">STATS =</span> means, <span class="dt">FUN =</span> <span class="st">`</span><span class="dt">-</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sweep</span>(<span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">STATS =</span> sds, <span class="dt">FUN =</span> <span class="st">`</span><span class="dt">/</span><span class="st">`</span>)
<span class="kw">MSE</span>(<span class="kw">predict</span>(ridge.<span class="dv">2</span>, <span class="dt">newx =</span> X.test.scaled)<span class="op">-</span><span class="st"> </span>y.test)</code></pre></div>
<pre><code>## [1] 0.7678264</code></pre>
<p>Things to note:</p>
<ul>
<li>The <code>alpha=0</code> parameters tells R to do ridge regression. Setting <span class="math inline">\(alpha=1\)</span> will do LASSO, and any other value, with return an elastic net with appropriate weights.</li>
<li>The <code>family='gaussian'</code> argument tells R to fit a linear model, with least squares loss.</li>
<li>Features for regularized predictors should be z-scored before learning.</li>
<li>We use the <code>sweep</code> function to z-score the predictors: we learn the z-scoring from the train set, and apply it to both the train and the test.</li>
<li>The test error is <strong>smaller</strong> than the train error. This may happen because risk estimators are random. Their variance may mask the overfitting.</li>
</ul>
<p>We now use the LASSO penalty.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x=</span>X.train.scaled, <span class="dt">y=</span>y.train, , <span class="dt">family=</span><span class="st">&#39;gaussian&#39;</span>, <span class="dt">alpha =</span> <span class="dv">1</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(lasso.<span class="dv">1</span>, <span class="dt">newx =</span>X.train.scaled)<span class="op">-</span><span class="st"> </span>y.train)</code></pre></div>
<pre><code>## [1] 0.5525279</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(lasso.<span class="dv">1</span>, <span class="dt">newx =</span> X.test.scaled)<span class="op">-</span><span class="st"> </span>y.test)</code></pre></div>
<pre><code>## [1] 0.5211263</code></pre>
<p>We now use <code>glmnet</code> for classification.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">means.spam &lt;-<span class="st"> </span><span class="kw">apply</span>(X.train.spam, <span class="dv">2</span>, mean)
sds.spam &lt;-<span class="st"> </span><span class="kw">apply</span>(X.train.spam, <span class="dv">2</span>, sd)
X.train.spam.scaled &lt;-<span class="st"> </span>X.train.spam <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sweep</span>(<span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">STATS =</span> means.spam, <span class="dt">FUN =</span> <span class="st">`</span><span class="dt">-</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sweep</span>(<span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">STATS =</span> sds.spam, <span class="dt">FUN =</span> <span class="st">`</span><span class="dt">/</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span>as.matrix

logistic.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="dt">x=</span>X.train.spam.scaled, <span class="dt">y=</span>y.train.spam, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">alpha =</span> <span class="dv">0</span>)</code></pre></div>
<p>Things to note:</p>
<ul>
<li>We used <code>cv.glmnet</code> to do an automatic search for the optimal level of regularization (the <code>lambda</code> argument in <code>glmnet</code>) using V-fold CV.</li>
<li>Just like the <code>glm</code> function, <code>'family='binomial'</code> is used for logistic regression.</li>
<li>We z-scored features so that they all have the same scale.</li>
<li>We set <code>alpha=0</code> for an <span class="math inline">\(l_2\)</span> penalization of the coefficients of the logistic regression.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(logistic.<span class="dv">2</span>, <span class="dt">newx =</span> X.train.spam.scaled, <span class="dt">type =</span> <span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1778  167
##      spam     66 1040</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train misclassification error</span>
<span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.0763684</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
X.test.spam.scaled &lt;-<span class="st"> </span>X.test.spam <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sweep</span>(<span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">STATS =</span> means.spam, <span class="dt">FUN =</span> <span class="st">`</span><span class="dt">-</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sweep</span>(<span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">STATS =</span> sds.spam, <span class="dt">FUN =</span> <span class="st">`</span><span class="dt">/</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span>as.matrix

.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(logistic.<span class="dv">2</span>, <span class="dt">newx =</span> X.test.spam.scaled, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>y.test.spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   885  110
##      spam     59  496</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test misclassification error:</span>
<span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1090323</code></pre>
</div>
<div id="svm" class="section level3">
<h3><span class="header-section-number">10.2.2</span> SVM</h3>
<p>A support vector machine (SVM) is a linear hypothesis class with a particular loss function known as a <a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a>. We learn an SVM with the <code>svm</code> function from the <strong>e1071</strong> package, which is merely a wrapper for the <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> C library; the most popular implementation of SVM today.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
svm.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">svm</span>(spam<span class="op">~</span>., <span class="dt">data =</span> spam.train, <span class="dt">kernel=</span><span class="st">&#39;linear&#39;</span>)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(svm.<span class="dv">1</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1774  106
##      spam     70 1101</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.057686</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(svm.<span class="dv">1</span>, <span class="dt">newdata =</span> spam.test) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   876   75
##      spam     68  531</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.09225806</code></pre>
<p>We can also use SVM for regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">svm</span>(lcavol<span class="op">~</span>., <span class="dt">data =</span> prostate.train, <span class="dt">kernel=</span><span class="st">&#39;linear&#39;</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(svm.<span class="dv">2</span>)<span class="op">-</span><span class="st"> </span>prostate.train<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.4488577</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(svm.<span class="dv">2</span>, <span class="dt">newdata =</span> prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.5547759</code></pre>
<p>Things to note:</p>
<ul>
<li>The use of <code>kernel='linear'</code> forces the predictor to be linear. Various hypothesis classes may be used by changing the <code>kernel</code> argument.</li>
</ul>
</div>
<div id="neural-nets" class="section level3">
<h3><span class="header-section-number">10.2.3</span> Neural Nets</h3>
<p>Neural nets (non deep) can be fitted, for example, with the <code>nnet</code> function in the <strong>nnet</strong> package. We start with a nnet regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nnet)
nnet.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">nnet</span>(lcavol<span class="op">~</span>., <span class="dt">size=</span><span class="dv">20</span>, <span class="dt">data=</span>prostate.train, <span class="dt">rang =</span> <span class="fl">0.1</span>, <span class="dt">decay =</span> <span class="fl">5e-4</span>, <span class="dt">maxit =</span> <span class="dv">1000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(nnet.<span class="dv">1</span>)<span class="op">-</span><span class="st"> </span>prostate.train<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 1.177099</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(nnet.<span class="dv">1</span>, <span class="dt">newdata =</span> prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 1.21175</code></pre>
<p>And nnet classification.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nnet.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">nnet</span>(spam<span class="op">~</span>., <span class="dt">size=</span><span class="dv">5</span>, <span class="dt">data=</span>spam.train, <span class="dt">rang =</span> <span class="fl">0.1</span>, <span class="dt">decay =</span> <span class="fl">5e-4</span>, <span class="dt">maxit =</span> <span class="dv">1000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(nnet.<span class="dv">2</span>, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1806   59
##      spam     38 1148</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.03179285</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(nnet.<span class="dv">2</span>, <span class="dt">newdata =</span> spam.test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   897   64
##      spam     47  542</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.0716129</code></pre>
<div id="deep-neural-nets" class="section level4">
<h4><span class="header-section-number">10.2.3.1</span> Deep Neural Nets</h4>
<p>Deep-Neural-Networks are undoubtedly the “hottest” topic in machine-learning and artificial intelligence. This real is too vast to be covered in this text. We merely refer the reader to the <a href="https://cran.r-project.org/package=tensorflow">tensorflow</a> package documentation as a starting point.</p>
</div>
</div>
<div id="trees" class="section level3">
<h3><span class="header-section-number">10.2.4</span> Classification and Regression Trees (CART)</h3>
<p>A CART, is not a linear hypothesis class. It partitions the feature space <span class="math inline">\(\mathcal{X}\)</span>, thus creating a set of if-then rules for prediction or classification. It is thus particularly useful when you believe that the predicted classes may change abruptly with small changes in <span class="math inline">\(x\)</span>.</p>
<div id="rpart" class="section level4">
<h4><span class="header-section-number">10.2.4.1</span> The rpart Package</h4>
<p>This view clarifies the name of the function <code>rpart</code>, which <em>recursively partitions</em> the feature space.</p>
<p>We start with a regression tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;rpart&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:survival&#39;:
## 
##     solder</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tree.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(lcavol<span class="op">~</span>., <span class="dt">data=</span>prostate.train)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(tree.<span class="dv">1</span>)<span class="op">-</span><span class="st"> </span>prostate.train<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.4909568</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(tree.<span class="dv">1</span>, <span class="dt">newdata =</span> prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.5623316</code></pre>
<p>We can use the <strong>rpart.plot</strong> package to visualize and interpret the predictor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rpart.plot<span class="op">::</span><span class="kw">rpart.plot</span>(tree.<span class="dv">1</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-244-1.png" width="50%" /></p>
<p>Trees are very prone to overfitting. To avoid this, we reduce a tree’s complexity by <em>pruning</em> it. This is done with the <code>rpart::prune</code> function (not demonstrated herein).</p>
<p>We now fit a classification tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tree.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(spam<span class="op">~</span>., <span class="dt">data=</span>spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(tree.<span class="dv">2</span>, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1785  217
##      spam     59  990</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.09046214</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(tree.<span class="dv">2</span>, <span class="dt">newdata =</span> spam.test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   906  125
##      spam     38  481</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1051613</code></pre>
</div>
<div id="caret" class="section level4">
<h4><span class="header-section-number">10.2.4.2</span> The caret Package</h4>
<p>In the <strong>rpart</strong> package [<a href="supervised.html#rpart">10.2.4.1</a>] we grow a tree with one function, and then prune it with another.<br />
The <strong>caret</strong> implementation of trees does both with a single function. We demonstrate the package in the context of trees, but it is actually a very convenient wrapper for many learning algorithms; <a href="http://topepo.github.io/caret/available-models.html#">237(!)</a> learning algorithms to be precise.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="co"># Control some training parameters</span>
train.control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
                           <span class="dt">number =</span> <span class="dv">10</span>)

tree.<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">train</span>(lcavol<span class="op">~</span>., <span class="dt">data=</span>prostate.train, 
                <span class="dt">method=</span><span class="st">&#39;rpart&#39;</span>, 
                <span class="dt">trControl=</span>train.control)
tree.<span class="dv">3</span></code></pre></div>
<pre><code>## CART 
## 
## 67 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 61, 60, 59, 60, 60, 61, ... 
## Resampling results across tuning parameters:
## 
##   cp          RMSE       Rsquared   MAE      
##   0.04682924  0.9118374  0.5026786  0.7570798
##   0.14815712  0.9899308  0.4690557  0.7972803
##   0.44497285  1.1912870  0.3264172  1.0008574
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was cp = 0.04682924.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(tree.<span class="dv">3</span>)<span class="op">-</span><span class="st"> </span>prostate.train<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.6188435</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(tree.<span class="dv">3</span>, <span class="dt">newdata =</span> prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.545632</code></pre>
<p>Things to note:</p>
<ul>
<li>A tree was trained because of the <code>method='rpart'</code> argument. Many other predictive models are available. See <a href="http://topepo.github.io/caret/available-models.html">here</a>.</li>
<li>The pruning of the tree was done automatically by the <code>caret::train()</code> function.</li>
<li>The method of pruning is controlled by a control object, generated with the <code>caret::trainControl()</code> function. In our case, <code>method = &quot;cv&quot;</code> for cross-validation, and <code>number = 10</code> for 10-folds.</li>
<li>The train error is larger than the test error. This is possible because the tree is not an ERM on the train data. Rather, it is an ERM on the variations of the data generated by the cross-validation process.</li>
</ul>
</div>
</div>
<div id="k-nearest-neighbour-knn" class="section level3">
<h3><span class="header-section-number">10.2.5</span> K-nearest neighbour (KNN)</h3>
<p>KNN is not an ERM problem. In the KNN algorithm, a prediction at some <span class="math inline">\(x\)</span> is made based on the <span class="math inline">\(y\)</span> is it neighbors. This means that:</p>
<ul>
<li>KNN is an <a href="https://en.wikipedia.org/wiki/Instance-based_learning">Instance Based</a> learning algorithm where we do not learn the values of some parametric function, but rather, need the original sample to make predictions. This has many implications when dealing with “BigData”.</li>
<li>It may only be applied in spaces with known/defined metric. It is thus harder to apply in the presence of missing values, or in “string-spaces”, “genome-spaces”, etc. where no canonical metric exists.</li>
</ul>
<p>KNN is so fundamental that we show how to fit such a hypothesis class, even if it not an ERM algorithm. Is KNN any good? I have never seen a learning problem where KNN beats other methods. Others claim differently.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(class)
knn.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train =</span> X.train.spam.scaled, <span class="dt">test =</span> X.test.spam.scaled, <span class="dt">cl =</span>y.train.spam, <span class="dt">k =</span> <span class="dv">1</span>)

<span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span>knn.<span class="dv">1</span> 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   856   86
##      spam     88  520</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1122581</code></pre>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3">
<h3><span class="header-section-number">10.2.6</span> Linear Discriminant Analysis (LDA)</h3>
<p>LDA is equivalent to least squares classification <a href="supervised.html#least-squares">10.2.1</a>. This means that we actually did LDA when we used <code>lm</code> for binary classification (feel free to compare the confusion matrices). There are, however, some dedicated functions to fit it which we now introduce.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
lda.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(spam<span class="op">~</span>., spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(lda.<span class="dv">1</span>)<span class="op">$</span>class
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1776  227
##      spam     68  980</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.09668961</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(lda.<span class="dv">1</span>, <span class="dt">newdata =</span> spam.test)<span class="op">$</span>class
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   884  138
##      spam     60  468</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1277419</code></pre>
</div>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">10.2.7</span> Naive Bayes</h3>
<p>Naive-Bayes can be thought of LDA, i.e. linear regression, where predictors are assume to be uncorrelated. Predictions may be very good and certainly very fast, even if this assumption is not true.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
nb.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(spam<span class="op">~</span>., <span class="dt">data =</span> spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(nb.<span class="dv">1</span>, <span class="dt">newdata =</span> spam.train)
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1025   55
##      spam    819 1152</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.2864635</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(nb.<span class="dv">1</span>, <span class="dt">newdata =</span> spam.test)
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   484   42
##      spam    460  564</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.323871</code></pre>
</div>
<div id="random-forrest" class="section level3">
<h3><span class="header-section-number">10.2.8</span> Random Forrest</h3>
<p>A Random Forrest is one of the most popular supervised learning algorithms. It it an extremely successful algorithm, with very few tuning parameters, and easily parallelizable (thus salable to massive datasets).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Control some training parameters</span>
train.control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
rf.<span class="dv">1</span> &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(lcavol<span class="op">~</span>., <span class="dt">data=</span>prostate.train, 
                <span class="dt">method=</span><span class="st">&#39;rf&#39;</span>, 
                <span class="dt">trControl=</span>train.control)
rf.<span class="dv">1</span></code></pre></div>
<pre><code>## Random Forest 
## 
## 67 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 62, 59, 60, 60, 59, 61, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE      
##   2     0.7885535  0.6520820  0.6684168
##   5     0.7782809  0.6687843  0.6550590
##   8     0.7894338  0.6665277  0.6626417
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 5.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(rf.<span class="dv">1</span>)<span class="op">-</span><span class="st"> </span>prostate.train<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.1340291</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(rf.<span class="dv">1</span>, <span class="dt">newdata =</span> prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.5147782</code></pre>
<p>Some of the many many many packages that learn random-forests include: <a href="https://cran.r-project.org/package=randomForest">randomForest</a>, <a href="https://cran.r-project.org/package=ranger">ranger</a>.</p>
</div>
<div id="boosting" class="section level3">
<h3><span class="header-section-number">10.2.9</span> Boosting</h3>
<p>The fundamental idea behind <strong>Boosting</strong> is to construct a predictor, as the sum of several “weak” predictors. These weak predictors, are not trained on the same data. Instead, each predictor is trained on the residuals of the previous. Think of it this way: The first predictor targets the strongest signal. The second targets what the first did not predict. Etc. At some point, the residuals cannot be predicted anymore, and the learning will stabilize. Boosting is typically, but not necessarily, implemented as a sum of trees (@(trees)).</p>
<div id="the-gbm-package" class="section level4">
<h4><span class="header-section-number">10.2.9.1</span> The gbm Package</h4>
<p>TODO</p>
</div>
<div id="the-xgboost-package" class="section level4">
<h4><span class="header-section-number">10.2.9.2</span> The xgboost Package</h4>
<p>TODO</p>
</div>
</div>
</div>
<div id="bibliographic-notes-8" class="section level2">
<h2><span class="header-section-number">10.3</span> Bibliographic Notes</h2>
<p>The ultimate reference on (statistical) machine learning is <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span>. For a softer introduction, see <span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span>. A statistician will also like <span class="citation">Ripley (<a href="#ref-ripley2007pattern">2007</a>)</span>. For a very algorithmic view, see the seminal <span class="citation">Leskovec, Rajaraman, and Ullman (<a href="#ref-leskovec2014mining">2014</a>)</span> or <span class="citation">Conway and White (<a href="#ref-conway2012machine">2012</a>)</span>. For a much more theoretical reference, see <span class="citation">Mohri, Rostamizadeh, and Talwalkar (<a href="#ref-mohri2012foundations">2012</a>)</span>, <span class="citation">Vapnik (<a href="#ref-vapnik2013nature">2013</a>)</span>, <span class="citation">Shalev-Shwartz and Ben-David (<a href="#ref-shalev2014understanding">2014</a>)</span>. Terminology taken from <span class="citation">Sammut and Webb (<a href="#ref-sammut2011encyclopedia">2011</a>)</span>. For an R oriented view see <span class="citation">Lantz (<a href="#ref-lantz2013machine">2013</a>)</span>. For review of other R sources for machine learning see <a href="http://modernstatisticalworkflow.blogspot.com/2018/01/some-good-introductory-machine-learning.html">Jim Savege’s post</a>, or the official <a href="https://cran.r-project.org/web/views/MachineLearning.html">Task View</a>. For a review of resampling based unbiased risk estimation (i.e. cross validation) see the exceptional review of <span class="citation">Arlot, Celisse, and others (<a href="#ref-arlot2010survey">2010</a>)</span>. If you want to know about Deep-Nets in R see <a href="https://www.datacamp.com/community/tutorials/keras-r-deep-learning">here</a>.</p>
</div>
<div id="practice-yourself-6" class="section level2">
<h2><span class="header-section-number">10.4</span> Practice Yourself</h2>
<ol style="list-style-type: decimal">
<li>In <a href="glm.html#practice-glm">7.6</a> we fit a GLM for the <code>MASS::epil</code> data (Poisson family). We assume that the number of seizures (<span class="math inline">\(y\)</span>) depending on the age of the patient (<code>age</code>) and the treatment (<code>trt</code>).
<ol style="list-style-type: decimal">
<li>What was the MSE of the model?</li>
<li>Now, try the same with a ridge penalty using <code>glmnet</code> (<code>alpha=0</code>).</li>
<li>Do the same with a LASSO penalty (<code>alpha=1</code>).</li>
<li>Compare the test MSE of the three models. Which is the best ?</li>
</ol></li>
<li>Read about the <code>Glass</code> dataset using <code>data(Glass, package=&quot;mlbench&quot;)</code> and <code>?Glass</code>.
<ol style="list-style-type: decimal">
<li>Divide the dataset to train set and test set.</li>
<li>Apply the various predictors from this chapter, and compare them using the proportion of missclassified.</li>
</ol></li>
</ol>
<p>See DataCamp’s <a href="https://www.datacamp.com/courses/supervised-learning-in-r-classification">Supervised Learning in R: Classification</a>, and <a href="https://www.datacamp.com/courses/supervised-learning-in-r-regression">Supervised Learning in R: Regression</a> for more self practice.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-arlot2010survey">
<p>Arlot, Sylvain, Alain Celisse, and others. 2010. “A Survey of Cross-Validation Procedures for Model Selection.” <em>Statistics Surveys</em> 4. The author, under a Creative Commons Attribution License: 40–79.</p>
</div>
<div id="ref-conway2012machine">
<p>Conway, Drew, and John White. 2012. <em>Machine Learning for Hackers</em>. “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-foster2004variable">
<p>Foster, Dean P, and Robert A Stine. 2004. “Variable Selection in Data Mining: Building a Predictive Model for Bankruptcy.” <em>Journal of the American Statistical Association</em> 99 (466). Taylor &amp; Francis: 303–13.</p>
</div>
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. Springer series in statistics Springer, Berlin.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 6. Springer.</p>
</div>
<div id="ref-lantz2013machine">
<p>Lantz, Brett. 2013. <em>Machine Learning with R</em>. Packt Publishing Ltd.</p>
</div>
<div id="ref-leskovec2014mining">
<p>Leskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. <em>Mining of Massive Datasets</em>. Cambridge University Press.</p>
</div>
<div id="ref-mohri2012foundations">
<p>Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. <em>Foundations of Machine Learning</em>. MIT press.</p>
</div>
<div id="ref-ripley2007pattern">
<p>Ripley, Brian D. 2007. <em>Pattern Recognition and Neural Networks</em>. Cambridge university press.</p>
</div>
<div id="ref-sammut2011encyclopedia">
<p>Sammut, Claude, and Geoffrey I Webb. 2011. <em>Encyclopedia of Machine Learning</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-shalev2014understanding">
<p>Shalev-Shwartz, Shai, and Shai Ben-David. 2014. <em>Understanding Machine Learning: From Theory to Algorithms</em>. Cambridge university press.</p>
</div>
<div id="ref-vapnik2013nature">
<p>Vapnik, Vladimir. 2013. <em>The Nature of Statistical Learning Theory</em>. Springer science &amp; business media.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p>It is even a subset of the Hilbert space, itself a subset of the space of all functions.<a href="supervised.html#fnref19">↩</a></p></li>
<li id="fn20"><p>Example taken from <a href="https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/ch6.html" class="uri">https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/ch6.html</a><a href="supervised.html#fnref20">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multivariate.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="plotting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/55-supervised.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Rcourse.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
